{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidteng/anaconda3/envs/wsd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, torch\n",
    "import lightning  as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.nn import functional as F\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using GPU: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "['0', '1013.49', '26.46', '42.99', '18575.83', '61.29']\n"
     ]
    }
   ],
   "source": [
    "train_data_paths = [f\"../36_TrainingData/L{loc}_Train.csv\" for loc in range(1, 17+1)]\n",
    "add_ids = [2,4,7,8,9,10,12]\n",
    "add_train_data_paths = [f\"../36_TrainingData_Additional_V2/L{loc}_Train_2.csv\" for loc in add_ids]\n",
    "\n",
    "train_data_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for csv_path in train_data_paths:\n",
    "    with open(csv_path, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        header = next(reader)  # 跳過標題列\n",
    "        for row in reader:\n",
    "            # 解析數據並存入字典\n",
    "            loc = row[0]\n",
    "            date = row[1][:10]\n",
    "            time = row[1][11:16]\n",
    "            train_data_dict[loc][date][time] = row[2:]\n",
    "print(len(train_data_dict['2']))\n",
    "for csv_path in add_train_data_paths:\n",
    "    with open(csv_path, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        header = next(reader)  # 跳過標題列\n",
    "        for row in reader:\n",
    "            # 解析數據並存入字典\n",
    "            loc = row[0]\n",
    "            date = row[1][:10]\n",
    "            time = row[1][11:16]\n",
    "            train_data_dict[loc][date][time] = row[2:]\n",
    "print((train_data_dict['2']['2024-01-17']['15:28']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load cwb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0']\n"
     ]
    }
   ],
   "source": [
    "months = ['01','02','03','04','05','06','07','08', '09', '10']\n",
    "features = ['rain', 'raintime', 'solarpower', 'suntime', 'temp', 'uv']\n",
    "cwb_data_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "for month in months:\n",
    "    for feature in features:\n",
    "        with open(f'../cwbdata/{month}/{feature}-{month}.csv', 'r', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            for row in reader:\n",
    "                if row[0].isdigit():\n",
    "                    cwb_data_dict[feature][month][row[0]] = row[1:]\n",
    "print(cwb_data_dict['rain']['01']['01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cwb2LocDataset(Dataset):\n",
    "    def __init__(self, train_data_dict, cwb_data_dict, loc, max_len=700):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for location, loc_dict in train_data_dict.items():\n",
    "            # if location != loc:\n",
    "            #     continue\n",
    "            for date, date_dict in loc_dict.items():\n",
    "                train_date_x = []\n",
    "                train_date_y = []\n",
    "                month = date[5:7]\n",
    "                day = date[8:10]\n",
    "                for time, values in date_dict.items():\n",
    "                    hour = time[0:2]\n",
    "                    try:\n",
    "                        # 確保索引不超出範圍，逐項處理每個特徵值\n",
    "                        newx = [\n",
    "                            float(cwb_data_dict['rain'][month][day][int(hour) + 1]) if month in cwb_data_dict['rain'] and day in cwb_data_dict['rain'][month] and int(hour) + 1 < len(cwb_data_dict['rain'][month][day]) else 0.0,\n",
    "                            float(cwb_data_dict['raintime'][month][day][int(hour) + 1]) if month in cwb_data_dict['raintime'] and day in cwb_data_dict['raintime'][month] and int(hour) + 1 < len(cwb_data_dict['raintime'][month][day]) else 0.0,\n",
    "                            float(cwb_data_dict['solarpower'][month][day][int(hour) + 1]) if month in cwb_data_dict['solarpower'] and day in cwb_data_dict['solarpower'][month] and int(hour) + 1 < len(cwb_data_dict['solarpower'][month][day]) else 0.0,\n",
    "                            float(cwb_data_dict['suntime'][month][day][int(hour) + 1]) if month in cwb_data_dict['suntime'] and day in cwb_data_dict['suntime'][month] and int(hour) + 1 < len(cwb_data_dict['suntime'][month][day]) else 0.0,\n",
    "                            float(cwb_data_dict['temp'][month][day][int(hour) + 1]) if month in cwb_data_dict['temp'] and day in cwb_data_dict['temp'][month] and int(hour) + 1 < len(cwb_data_dict['temp'][month][day]) else 0.0,\n",
    "                            float(cwb_data_dict['uv'][month][day][int(hour) + 1]) if month in cwb_data_dict['uv'] and day in cwb_data_dict['uv'][month] and int(hour) + 1 < len(cwb_data_dict['uv'][month][day]) else 0.0,\n",
    "                            int(location)\n",
    "                        ]\n",
    "                    except (IndexError, KeyError, ValueError, TypeError):\n",
    "                        newx = [0.0] * 7  # 這部分只在其他未知錯誤時填充全 0\n",
    "\n",
    "                    newy = self.str2float(values[-1:])\n",
    "                    train_date_x.append(newx)\n",
    "                    train_date_y.append(newy)\n",
    "                \n",
    "                # 填充到 max_len\n",
    "                if len(train_date_x) < max_len:\n",
    "                    padding_length = max_len - len(train_date_x)\n",
    "                    train_date_x.extend([[0.0] * 7] * padding_length)\n",
    "                    train_date_y.extend([[0.0] * 1] * padding_length)\n",
    "                else:\n",
    "                    train_date_x = train_date_x[:max_len]\n",
    "                    train_date_y = train_date_y[:max_len]\n",
    "                \n",
    "                self.x.append(train_date_x)\n",
    "                self.y.append(train_date_y)\n",
    "        \n",
    "        # 檢查所有樣本長度是否一致\n",
    "        for sample_x in self.x:\n",
    "            assert len(sample_x) == max_len, f\"Sample X length {len(sample_x)} != {max_len}\"\n",
    "        for sample_y in self.y:\n",
    "            assert len(sample_y) == max_len, f\"Sample Y length {len(sample_y)} != {max_len}\"\n",
    "        \n",
    "        # 將數據轉換為 NumPy 數組\n",
    "        self.x = np.array(self.x, dtype=np.float32)  # 形狀: (num_samples, max_len, 6)\n",
    "        self.y = np.array(self.y, dtype=np.float32)  # 形狀: (num_samples, max_len, 4)\n",
    "        \n",
    "        # 初始化標準化器\n",
    "        self.x_scaler = StandardScaler()\n",
    "        self.y_scaler = StandardScaler()\n",
    "        \n",
    "        # 重塑數據以適應 StandardScaler 的輸入要求 (num_samples * max_len, num_features)\n",
    "        num_samples, max_len_x, num_features_x = self.x.shape\n",
    "        num_samples_y, max_len_y, num_features_y = self.y.shape\n",
    "        assert max_len_x == max_len_y, \"x 和 y 的序列長度不一致\"\n",
    "        \n",
    "        self.x = self.x.reshape(-1, num_features_x)\n",
    "        self.y = self.y.reshape(-1, num_features_y)\n",
    "        \n",
    "        # 擬合並轉換數據\n",
    "        self.x_scaler.fit(self.x)\n",
    "        self.y_scaler.fit(self.y)\n",
    "        \n",
    "        self.x = self.x_scaler.transform(self.x).reshape(num_samples, max_len_x, num_features_x)\n",
    "        self.y = self.y_scaler.transform(self.y).reshape(num_samples, max_len_y, num_features_y)\n",
    "        \n",
    "        # 將數據轉換回列表以節省內存（可選）\n",
    "        # self.x = self.x.tolist()\n",
    "        # self.y = self.y.tolist()\n",
    "\n",
    "        # 保存標準化器\n",
    "        with open(f'./scalar/x_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.x_scaler, f)\n",
    "\n",
    "        with open(f'./scalar/y_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.y_scaler, f)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x[index], dtype=torch.float32), torch.tensor(self.y[index], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def str2float(self, string_list):\n",
    "        float_list = []\n",
    "        for x in string_list:\n",
    "            try:\n",
    "                float_list.append(float(x))\n",
    "            except (ValueError, TypeError):\n",
    "                float_list.append(0.0)\n",
    "        return float_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = Cwb2LocDataset(\n",
    "#     train_data_dict,\n",
    "#     cwb_data_dict,\n",
    "#     loc='1',\n",
    "# )\n",
    "# print(test[1][0][:10]) # train_x\n",
    "# print(test[1][1][:10]) # train_y\n",
    "# print(len(test[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cut data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cwb2LocDataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_data_dict, cwb_data_dict, loc, batch_size=16, train_split=0.9):\n",
    "        super().__init__()\n",
    "        self.train_data_dict = train_data_dict\n",
    "        self.cwb_data_dict = cwb_data_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.train_split = train_split\n",
    "        self.loc = loc\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = Cwb2LocDataset(self.train_data_dict, self.cwb_data_dict, str(self.loc))\n",
    "        train_size = int(len(dataset) * self.train_split)\n",
    "        val_size = len(dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_datamodule = Cwb2LocDataModule(train_data_dict, cwb_data_dict, 1)\n",
    "# test_datamodule.setup()\n",
    "# train_loader = test_datamodule.train_dataloader()\n",
    "# val_loader = test_datamodule.val_dataloader()\n",
    "\n",
    "# print(f\"Train batches: {len(train_loader)}\")\n",
    "# print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# # 測試加載數據\n",
    "# for batch in train_loader:\n",
    "#     x, y = batch\n",
    "#     print(f\"Train batch features shape: {x.shape}\")\n",
    "#     print(f\"Train batch labels shape: {y.shape}\")\n",
    "#     break\n",
    "\n",
    "# for batch in val_loader:\n",
    "#     x, y = batch\n",
    "#     print(f\"Validation batch features shape: {x.shape}\")\n",
    "#     print(f\"Validation batch labels shape: {y.shape}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cwb2LocModel(L.LightningModule):\n",
    "    def __init__(self, input_dim=7, hidden_dim=128, num_layers=2, output_dim=1, learning_rate=1e-3, delta=1.0):\n",
    "        super(Cwb2LocModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.delta = delta  # 動態設置 delta\n",
    "\n",
    "        # 定義 LSTM 層\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "\n",
    "        # 定義全連接層\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim // 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim // 4, input_dim)\n",
    "        self.fc3 = nn.Linear(input_dim, 1)\n",
    "\n",
    "        # 定義損失函數（Huber Loss）\n",
    "        # self.criterion = nn.HuberLoss(delta=self.delta)\n",
    "        # self.criterion = nn.MSELoss()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        # self.criterion = log_cosh_loss\n",
    "\n",
    "    def log_cosh_loss(y_pred, y_true):\n",
    "        loss = torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 初始化隱藏狀態和細胞狀態\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(self.device)\n",
    "        residual = x\n",
    "        \n",
    "        # 前向傳播 LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # 通過全連接層得到最終輸出\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out += residual\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)  # 使用 Huber Loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)  # 使用 Huber Loss\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=1e-2\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.9)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate=1e-3\n",
    "# loc = 2\n",
    "hidden_dim = 128\n",
    "lstm_layers = 2\n",
    "data_module = Cwb2LocDataModule(train_data_dict, cwb_data_dict, loc, batch_size)\n",
    "model = Cwb2LocModel(\n",
    "    input_dim=7,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=lstm_layers,\n",
    "    output_dim=1,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "wandb_name = f'v4_all_b{batch_size}_h{hidden_dim}_ll{lstm_layers}_lr{learning_rate}_雙向_mae_p16_3fc_test00'\n",
    "# dirpath=f'./saved_models/{loc}/{wandb_name}'\n",
    "# dirpath=f'./saved_models/{loc}'\n",
    "dirpath=f'./saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "logger = WandbLogger(\n",
    "    project='aicup_power_v2', \n",
    "    name=wandb_name,\n",
    "    save_dir=None,\n",
    "    offline=False,\n",
    "    log_model=False,  # 不保存模型\n",
    "    save_code=False   # 不保存代碼快照\n",
    ")\n",
    "callbacks = [\n",
    "    LearningRateMonitor(),\n",
    "    ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        auto_insert_metric_name=False,\n",
    "        dirpath=dirpath,\n",
    "        filename='best-checkpoint-res-2-128-mae',\n",
    "        save_top_k=1,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',   # 監控的指標\n",
    "        patience=50,           # 如果 3 個 epoch 驗證損失沒有改善，則停止訓練\n",
    "        verbose=True,\n",
    "        mode='min'            # 損失越小越好\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    precision=32,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=1,\n",
    "    max_epochs=10000,\n",
    "    val_check_interval=1.0,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavid___teng\u001b[0m (\u001b[33mdavid___teng-national-central-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/davidteng/aicup/electric/code/wandb/run-20241125_232942-icitoeze</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/david___teng-national-central-university/aicup_power_v2/runs/icitoeze' target=\"_blank\">v4_all_b16_h128_ll2_lr0.001_雙向_mae_p16_3fc_test00</a></strong> to <a href='https://wandb.ai/david___teng-national-central-university/aicup_power_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/david___teng-national-central-university/aicup_power_v2' target=\"_blank\">https://wandb.ai/david___teng-national-central-university/aicup_power_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/david___teng-national-central-university/aicup_power_v2/runs/icitoeze' target=\"_blank\">https://wandb.ai/david___teng-national-central-university/aicup_power_v2/runs/icitoeze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidteng/anaconda3/envs/wsd/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/davidteng/aicup/electric/code/saved_models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type   | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | lstm      | LSTM   | 535 K  | train\n",
      "1 | fc1       | Linear | 8.2 K  | train\n",
      "2 | fc2       | Linear | 231    | train\n",
      "3 | fc3       | Linear | 8      | train\n",
      "4 | criterion | L1Loss | 0      | train\n",
      "---------------------------------------------\n",
      "544 K     Trainable params\n",
      "0         Non-trainable params\n",
      "544 K     Total params\n",
      "2.176     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidteng/anaconda3/envs/wsd/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidteng/anaconda3/envs/wsd/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 121/121 [00:03<00:00, 31.89it/s, v_num=oeze, train_loss_step=0.305, val_loss=0.366, train_loss_epoch=0.446]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 121/121 [00:03<00:00, 33.31it/s, v_num=oeze, train_loss_step=0.163, val_loss=0.332, train_loss_epoch=0.335]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.034 >= min_delta = 0.0. New best score: 0.332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 121/121 [00:03<00:00, 33.17it/s, v_num=oeze, train_loss_step=0.257, val_loss=0.329, train_loss_epoch=0.315]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 121/121 [00:03<00:00, 33.32it/s, v_num=oeze, train_loss_step=0.268, val_loss=0.328, train_loss_epoch=0.312]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 121/121 [00:02<00:00, 48.19it/s, v_num=oeze, train_loss_step=0.344, val_loss=0.311, train_loss_epoch=0.301]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 121/121 [00:02<00:00, 47.97it/s, v_num=oeze, train_loss_step=0.145, val_loss=0.298, train_loss_epoch=0.289]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 121/121 [00:02<00:00, 47.57it/s, v_num=oeze, train_loss_step=0.315, val_loss=0.281, train_loss_epoch=0.278]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 121/121 [00:02<00:00, 47.71it/s, v_num=oeze, train_loss_step=0.123, val_loss=0.281, train_loss_epoch=0.283]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 121/121 [00:02<00:00, 47.75it/s, v_num=oeze, train_loss_step=0.374, val_loss=0.278, train_loss_epoch=0.269]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 121/121 [00:02<00:00, 48.37it/s, v_num=oeze, train_loss_step=0.230, val_loss=0.272, train_loss_epoch=0.269]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 121/121 [00:02<00:00, 47.98it/s, v_num=oeze, train_loss_step=0.180, val_loss=0.258, train_loss_epoch=0.253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 121/121 [00:02<00:00, 47.76it/s, v_num=oeze, train_loss_step=0.195, val_loss=0.236, train_loss_epoch=0.237] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.021 >= min_delta = 0.0. New best score: 0.236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 121/121 [00:02<00:00, 47.97it/s, v_num=oeze, train_loss_step=0.212, val_loss=0.233, train_loss_epoch=0.219]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 121/121 [00:02<00:00, 47.35it/s, v_num=oeze, train_loss_step=0.279, val_loss=0.229, train_loss_epoch=0.216]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 121/121 [00:02<00:00, 47.93it/s, v_num=oeze, train_loss_step=0.166, val_loss=0.226, train_loss_epoch=0.210] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 121/121 [00:02<00:00, 47.71it/s, v_num=oeze, train_loss_step=0.232, val_loss=0.221, train_loss_epoch=0.210]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 121/121 [00:02<00:00, 47.33it/s, v_num=oeze, train_loss_step=0.126, val_loss=0.217, train_loss_epoch=0.205]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 121/121 [00:02<00:00, 47.60it/s, v_num=oeze, train_loss_step=0.166, val_loss=0.217, train_loss_epoch=0.199]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 121/121 [00:02<00:00, 47.61it/s, v_num=oeze, train_loss_step=0.186, val_loss=0.205, train_loss_epoch=0.197] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 0.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 121/121 [00:02<00:00, 48.03it/s, v_num=oeze, train_loss_step=0.132, val_loss=0.205, train_loss_epoch=0.182] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 121/121 [00:02<00:00, 48.18it/s, v_num=oeze, train_loss_step=0.124, val_loss=0.200, train_loss_epoch=0.178] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 121/121 [00:02<00:00, 48.12it/s, v_num=oeze, train_loss_step=0.227, val_loss=0.199, train_loss_epoch=0.177] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 121/121 [00:02<00:00, 48.08it/s, v_num=oeze, train_loss_step=0.137, val_loss=0.197, train_loss_epoch=0.178]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 121/121 [00:02<00:00, 48.38it/s, v_num=oeze, train_loss_step=0.203, val_loss=0.197, train_loss_epoch=0.173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 121/121 [00:02<00:00, 47.76it/s, v_num=oeze, train_loss_step=0.120, val_loss=0.195, train_loss_epoch=0.166] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 121/121 [00:02<00:00, 47.97it/s, v_num=oeze, train_loss_step=0.102, val_loss=0.191, train_loss_epoch=0.162] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104: 100%|██████████| 121/121 [00:02<00:00, 47.68it/s, v_num=oeze, train_loss_step=0.155, val_loss=0.189, train_loss_epoch=0.158] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110: 100%|██████████| 121/121 [00:02<00:00, 48.02it/s, v_num=oeze, train_loss_step=0.154, val_loss=0.186, train_loss_epoch=0.153] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116: 100%|██████████| 121/121 [00:02<00:00, 48.38it/s, v_num=oeze, train_loss_step=0.121, val_loss=0.186, train_loss_epoch=0.149] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119: 100%|██████████| 121/121 [00:02<00:00, 47.75it/s, v_num=oeze, train_loss_step=0.199, val_loss=0.185, train_loss_epoch=0.149] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124: 100%|██████████| 121/121 [00:02<00:00, 48.14it/s, v_num=oeze, train_loss_step=0.119, val_loss=0.183, train_loss_epoch=0.142] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135: 100%|██████████| 121/121 [00:02<00:00, 47.54it/s, v_num=oeze, train_loss_step=0.0604, val_loss=0.183, train_loss_epoch=0.141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 121/121 [00:02<00:00, 47.84it/s, v_num=oeze, train_loss_step=0.191, val_loss=0.183, train_loss_epoch=0.134] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150: 100%|██████████| 121/121 [00:02<00:00, 47.59it/s, v_num=oeze, train_loss_step=0.199, val_loss=0.182, train_loss_epoch=0.131] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160: 100%|██████████| 121/121 [00:02<00:00, 47.69it/s, v_num=oeze, train_loss_step=0.118, val_loss=0.181, train_loss_epoch=0.127] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164: 100%|██████████| 121/121 [00:02<00:00, 47.78it/s, v_num=oeze, train_loss_step=0.137, val_loss=0.179, train_loss_epoch=0.124] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170: 100%|██████████| 121/121 [00:02<00:00, 47.80it/s, v_num=oeze, train_loss_step=0.0732, val_loss=0.178, train_loss_epoch=0.122]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176: 100%|██████████| 121/121 [00:02<00:00, 47.89it/s, v_num=oeze, train_loss_step=0.103, val_loss=0.176, train_loss_epoch=0.121] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226: 100%|██████████| 121/121 [00:02<00:00, 48.14it/s, v_num=oeze, train_loss_step=0.129, val_loss=0.177, train_loss_epoch=0.106] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 50 records. Best score: 0.176. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226: 100%|██████████| 121/121 [00:02<00:00, 48.06it/s, v_num=oeze, train_loss_step=0.129, val_loss=0.177, train_loss_epoch=0.106]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
