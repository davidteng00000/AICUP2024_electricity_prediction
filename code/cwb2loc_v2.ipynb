{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidteng/anaconda3/envs/wsd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, torch\n",
    "import lightning  as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.nn import functional as F\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.interpolate import interp1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using GPU: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.47\n"
     ]
    }
   ],
   "source": [
    "train_data_paths = [f\"../36_TrainingData/L{loc}_Train.csv\" for loc in range(1, 17+1)]\n",
    "add_ids = [2,4,7,8,9,10,12]\n",
    "add_train_data_paths = [f\"../36_TrainingData_Additional_V2/L{loc}_Train_2.csv\" for loc in add_ids]\n",
    "\n",
    "train_data_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list)))))\n",
    "\n",
    "for csv_path in train_data_paths:\n",
    "    with open(csv_path, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        header = next(reader)  # 跳過標題列\n",
    "        for row in reader:\n",
    "            # 解析數據並存入字典\n",
    "            loc = row[0]\n",
    "            date = row[1][:10]\n",
    "            month = date[5:7]\n",
    "            day = date[8:10]\n",
    "            time = row[1][11:16]\n",
    "            hour = time[:2]\n",
    "            minute = time[-2:]\n",
    "            train_data_dict[int(loc)][int(month)][int(day)][int(hour)][int(minute)] = float(row[-1])\n",
    "for csv_path in add_train_data_paths:\n",
    "    with open(csv_path, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        header = next(reader)  # 跳過標題列\n",
    "        for row in reader:\n",
    "            # 解析數據並存入字典\n",
    "            loc = row[0]\n",
    "            date = row[1][:10]\n",
    "            month = date[5:7]\n",
    "            day = date[8:10]\n",
    "            time = row[1][11:16]\n",
    "            hour = time[:2]\n",
    "            minute = time[-2:]\n",
    "            train_data_dict[int(loc)][int(month)][int(day)][int(hour)][int(minute)] = float(row[-1])\n",
    "print((train_data_dict[1][2][2][9][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load cwb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9954829742877003, 0.9874913134120913, 0.9794996525364841]\n",
      "[0.9715079916608751, 0.9635163307852679, 0.9555246699096589, 0.9475330090340517, 0.9395413481584427, 0.9315496872828355, 0.9235580264072265, 0.9155663655316193, 0.9075747046560103, 0.8995830437804031, 0.8915913829047959, 0.8835997220291869, 0.8756080611535797, 0.8676164002779707, 0.8596247394023635, 0.8516330785267545, 0.8436414176511473, 0.8356497567755383, 0.8276580958999311, 0.8196664350243221, 0.8116747741487149, 0.8036831132731059, 0.7956914523974987, 0.7876997915218897, 0.7797081306462825, 0.7717164697706735, 0.7637248088950663, 0.7557331480194573, 0.7477414871438501, 0.7397498262682412, 0.7317581653926339, 0.723766504517025, 0.7157748436414177, 0.7077831827658088, 0.6997915218902016, 0.6917998610145943, 0.6838082001389854, 0.6758165392633781, 0.6678248783877692, 0.659833217512162, 0.651841556636553, 0.6438498957609458, 0.6358582348853368, 0.6278665740097296, 0.6198749131341206, 0.6118832522585134, 0.6038915913829044, 0.5958999305072972, 0.5879082696316882, 0.579916608756081, 0.571924947880472, 0.5639332870048648, 0.5559416261292558, 0.5479499652536486, 0.5399583043780396, 0.5319666435024324, 0.5239749826268234, 0.5159833217512162, 0.5079916608756072, 0.5]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# 定义月份和特征\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10']\n",
    "features = ['rain', 'raintime', 'solarpower', 'suntime', 'temp', 'uv']\n",
    "\n",
    "# 初始化 cwb_data_dict，四层嵌套：特征 -> 月份 -> 天数 -> 时间索引\n",
    "# 时间索引初始为小时（0-23），插值后为分钟（0-1439）\n",
    "cwb_data_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "\n",
    "# 读取气象数据\n",
    "for month in months:\n",
    "    for feature in features:\n",
    "        csv_path = f'../cwbdata/{month}/{feature}-{month}.csv'\n",
    "        try:\n",
    "            with open(csv_path, 'r', encoding='utf-8') as csv_file:\n",
    "                reader = csv.reader(csv_file)\n",
    "                for row in reader:\n",
    "                    if row[0].isdigit():\n",
    "                        day = int(row[0])\n",
    "                        for hour_index, value in enumerate(row[1:], start=0):\n",
    "                            if hour_index < 24:\n",
    "                                try:\n",
    "                                    cwb_data_dict[feature][int(month)][day][hour_index] = float(value)\n",
    "                                except (ValueError, TypeError):\n",
    "                                    cwb_data_dict[feature][int(month)][day][hour_index] = None  # 标记为无效值\n",
    "        except FileNotFoundError:\n",
    "            print(f\"文件未找到: {csv_path}\")\n",
    "\n",
    "# 插值函数\n",
    "def interpolate_feature(feature_data):\n",
    "    \"\"\"\n",
    "    对某一天的一个特征进行插值，从小时级别扩展到分钟级别。\n",
    "    feature_data: dict, {hour: value} 表示该天的一个特征数据\n",
    "    返回插值后的 1440 个分钟值\n",
    "    \"\"\"\n",
    "    hours = sorted(feature_data.keys())  # 获取所有小时索引\n",
    "    values = [feature_data[hour] for hour in hours]  # 获取每小时的值\n",
    "\n",
    "    # 处理无效值：用前后有效值的平均值填充\n",
    "    for i in range(len(values)):\n",
    "        if values[i] is None:\n",
    "            prev_valid = next((values[j] for j in range(i - 1, -1, -1) if values[j] is not None), None)\n",
    "            next_valid = next((values[j] for j in range(i + 1, len(values)) if values[j] is not None), None)\n",
    "            if prev_valid is not None and next_valid is not None:\n",
    "                values[i] = (prev_valid + next_valid) / 2\n",
    "            elif prev_valid is not None:\n",
    "                values[i] = prev_valid\n",
    "            elif next_valid is not None:\n",
    "                values[i] = next_valid\n",
    "            else:\n",
    "                values[i] = 0.0  # 如果前后都没有有效值，填充为 0.0\n",
    "\n",
    "    # 线性插值\n",
    "    x = np.array(hours)  # 小时索引\n",
    "    y = np.array(values)  # 每小时的值\n",
    "    x_minute = np.linspace(0, 23, 1440)  # 每分钟的索引\n",
    "    interpolated_values = np.interp(x_minute, x, y)  # 插值到分钟级别\n",
    "\n",
    "    return interpolated_values\n",
    "\n",
    "# 对每个特征的每一天数据进行插值并更新到字典中\n",
    "for feature in features:\n",
    "    for month in cwb_data_dict[feature]:\n",
    "        for day in cwb_data_dict[feature][month]:\n",
    "            # 插值数据\n",
    "            hourly_data = cwb_data_dict[feature][month][day]\n",
    "            interpolated_values = interpolate_feature(hourly_data)\n",
    "            \n",
    "            # 将插值结果以每小时为单位存储到字典中\n",
    "            for hour in range(24):\n",
    "                start_idx = hour * 60\n",
    "                end_idx = (hour + 1) * 60\n",
    "                cwb_data_dict[feature][month][day][hour] = interpolated_values[start_idx:end_idx].tolist()\n",
    "\n",
    "# 验证插值结果\n",
    "for feature in features:\n",
    "    for month in cwb_data_dict[feature]:\n",
    "        for day in cwb_data_dict[feature][month]:\n",
    "            for hour in cwb_data_dict[feature][month][day]:\n",
    "                assert len(cwb_data_dict[feature][month][day][hour]) == 60, \\\n",
    "                    f\"插值结果错误：{feature}, 月份 {month}, 日期 {day}, 小时 {hour}\"\n",
    "                assert all(isinstance(value, float) for value in cwb_data_dict[feature][month][day][hour]), \\\n",
    "                    f\"存在非 float 值：{feature}, 月份 {month}, 日期 {day}, 小时 {hour}\"\n",
    "print((cwb_data_dict['rain'][2][22][22]))\n",
    "print((cwb_data_dict['rain'][2][22][23]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cwb2LocDataset(Dataset):\n",
    "    def __init__(self, train_data_dict, cwb_data_dict, features, max_len=700):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for location, loc_dict in train_data_dict.items():\n",
    "            # if location != loc:\n",
    "            #     continue\n",
    "            for month, day_dict in loc_dict.items():\n",
    "                for day, hour_dict in day_dict.items():\n",
    "                    train_date_x = []\n",
    "                    train_date_y = []\n",
    "                    for hour, min_dict in hour_dict.items():\n",
    "                        for minute, value in min_dict.items():\n",
    "                            \n",
    "                            try:\n",
    "                                # 確保索引不超出範圍，逐項處理每個特徵值\n",
    "                                newx = []\n",
    "                                for feature in features:\n",
    "                                    newx.append(cwb_data_dict[feature][month][day][hour][minute])\n",
    "                                newx.append(int(location))\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                                newx = [0.0] * (len(features)+1)  # 這部分只在其他未知錯誤時填充全 0\n",
    "                            newy = [value]\n",
    "                            train_date_x.append(newx)\n",
    "                            train_date_y.append(newy)\n",
    "                \n",
    "                    # 填充到 max_len\n",
    "                    if len(train_date_x) < max_len:\n",
    "                        padding_length = max_len - len(train_date_x)\n",
    "                        train_date_x.extend([[0.0] * (len(features) + 1)] * padding_length)\n",
    "                        train_date_y.extend([[0.0] * 1] * padding_length)\n",
    "                    else:\n",
    "                        train_date_x = train_date_x[:max_len]\n",
    "                        train_date_y = train_date_y[:max_len]\n",
    "                    \n",
    "                    self.x.append(train_date_x)\n",
    "                    self.y.append(train_date_y)\n",
    "        \n",
    "        # 檢查所有樣本長度是否一致\n",
    "        for sample_x in self.x:\n",
    "            assert len(sample_x) == max_len, f\"Sample X length {len(sample_x)} != {max_len}\"\n",
    "        for sample_y in self.y:\n",
    "            assert len(sample_y) == max_len, f\"Sample Y length {len(sample_y)} != {max_len}\"\n",
    "        \n",
    "        # 將數據轉換為 NumPy 數組\n",
    "        self.x = np.array(self.x, dtype=np.float32)\n",
    "        self.y = np.array(self.y, dtype=np.float32)\n",
    "        \n",
    "        # 初始化標準化器\n",
    "        self.x_scaler = StandardScaler()\n",
    "        self.y_scaler = StandardScaler()\n",
    "        \n",
    "        # 重塑數據以適應 StandardScaler 的輸入要求 (num_samples * max_len, num_features)\n",
    "        num_samples, max_len_x, num_features_x = self.x.shape\n",
    "        num_samples_y, max_len_y, num_features_y = self.y.shape\n",
    "        assert max_len_x == max_len_y, \"x 和 y 的序列長度不一致\"\n",
    "        \n",
    "        self.x = self.x.reshape(-1, num_features_x)\n",
    "        self.y = self.y.reshape(-1, num_features_y)\n",
    "        \n",
    "        # 擬合並轉換數據\n",
    "        self.x_scaler.fit(self.x)\n",
    "        self.y_scaler.fit(self.y)\n",
    "        \n",
    "        self.x = self.x_scaler.transform(self.x).reshape(num_samples, max_len_x, num_features_x)\n",
    "        self.y = self.y_scaler.transform(self.y).reshape(num_samples, max_len_y, num_features_y)\n",
    "        \n",
    "        # 將數據轉換回列表以節省內存（可選）\n",
    "        # self.x = self.x.tolist()\n",
    "        # self.y = self.y.tolist()\n",
    "\n",
    "        # 保存標準化器\n",
    "        with open(f'./scalar_v2/x_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.x_scaler, f)\n",
    "\n",
    "        with open(f'./scalar_v2/y_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.y_scaler, f)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x[index], dtype=torch.float32), torch.tensor(self.y[index], dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def str2float(self, string_list):\n",
    "        float_list = []\n",
    "        for x in string_list:\n",
    "            try:\n",
    "                float_list.append(float(x))\n",
    "            except (ValueError, TypeError):\n",
    "                float_list.append(0.0)\n",
    "        return float_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = Cwb2LocDataset(\n",
    "#     train_data_dict,\n",
    "#     cwb_data_dict,\n",
    "#     features=features\n",
    "# )\n",
    "# print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cut data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cwb2LocDataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_data_dict, cwb_data_dict, features, batch_size=16, train_split=0.9):\n",
    "        super().__init__()\n",
    "        self.train_data_dict = train_data_dict\n",
    "        self.cwb_data_dict = cwb_data_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.train_split = train_split\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = Cwb2LocDataset(self.train_data_dict, self.cwb_data_dict, features)\n",
    "        train_size = int(len(dataset) * self.train_split)\n",
    "        val_size = len(dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 121\n",
      "Validation batches: 14\n",
      "Train batch features shape: torch.Size([16, 700, 7])\n",
      "Train batch labels shape: torch.Size([16, 700, 1])\n",
      "Validation batch features shape: torch.Size([16, 700, 7])\n",
      "Validation batch labels shape: torch.Size([16, 700, 1])\n"
     ]
    }
   ],
   "source": [
    "test_datamodule = Cwb2LocDataModule(train_data_dict, cwb_data_dict, features)\n",
    "test_datamodule.setup()\n",
    "train_loader = test_datamodule.train_dataloader()\n",
    "val_loader = test_datamodule.val_dataloader()\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# 測試加載數據\n",
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    print(f\"Train batch features shape: {x.shape}\")\n",
    "    print(f\"Train batch labels shape: {y.shape}\")\n",
    "    break\n",
    "\n",
    "for batch in val_loader:\n",
    "    x, y = batch\n",
    "    print(f\"Validation batch features shape: {x.shape}\")\n",
    "    print(f\"Validation batch labels shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cwb2LocModel(L.LightningModule):\n",
    "    def __init__(self, input_dim=7, hidden_dim=128, num_layers=2, output_dim=1, learning_rate=1e-3, delta=1.0):\n",
    "        super(Cwb2LocModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.delta = delta  # 動態設置 delta\n",
    "\n",
    "        # 定義 LSTM 層\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "\n",
    "        # 定義全連接層\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim // 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim // 4, input_dim)\n",
    "        self.fc3 = nn.Linear(input_dim, 1)\n",
    "\n",
    "        # 定義損失函數（Huber Loss）\n",
    "        # self.criterion = nn.HuberLoss(delta=self.delta)\n",
    "        # self.criterion = nn.MSELoss()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        # self.criterion = log_cosh_loss\n",
    "\n",
    "    def log_cosh_loss(y_pred, y_true):\n",
    "        loss = torch.mean(torch.log(torch.cosh(y_pred - y_true)))\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 初始化隱藏狀態和細胞狀態\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(self.device)\n",
    "        residual = x\n",
    "        \n",
    "        # 前向傳播 LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # 通過全連接層得到最終輸出\n",
    "        out = self.fc1(out)\n",
    "        # out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out += residual\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)  # 使用 Huber Loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)  # 使用 Huber Loss\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=1e-2\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.9)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate=1e-3\n",
    "# loc = 2\n",
    "hidden_dim = 128\n",
    "lstm_layers = 2\n",
    "data_module = Cwb2LocDataModule(train_data_dict, cwb_data_dict, features, batch_size)\n",
    "model = Cwb2LocModel(\n",
    "    input_dim=7,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=lstm_layers,\n",
    "    output_dim=1,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "wandb_name = f'f6_b{batch_size}_h{hidden_dim}_ll{lstm_layers}_lr{learning_rate}_雙向_mae_p32_3fc_test00線性插值-D'\n",
    "# dirpath=f'./saved_models/{loc}/{wandb_name}'\n",
    "# dirpath=f'./saved_models/{loc}'\n",
    "dirpath=f'./saved_models_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "logger = WandbLogger(\n",
    "    project='aicup_power_v3', \n",
    "    name=wandb_name,\n",
    "    save_dir=None,\n",
    "    offline=False,\n",
    "    log_model=False,  # 不保存模型\n",
    "    save_code=False   # 不保存代碼快照\n",
    ")\n",
    "callbacks = [\n",
    "    LearningRateMonitor(),\n",
    "    ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        auto_insert_metric_name=False,\n",
    "        dirpath=dirpath,\n",
    "        filename='best-checkpoint-6-C',\n",
    "        save_top_k=1,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',   # 監控的指標\n",
    "        patience=50,           # 如果 3 個 epoch 驗證損失沒有改善，則停止訓練\n",
    "        verbose=True,\n",
    "        mode='min'            # 損失越小越好\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    precision=32,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=1,\n",
    "    max_epochs=10000,\n",
    "    val_check_interval=1.0,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavid___teng\u001b[0m (\u001b[33mdavid___teng-national-central-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/davidteng/aicup/electric/code/wandb/run-20241126_033242-l7k80jst</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/david___teng-national-central-university/aicup_power_v3/runs/l7k80jst' target=\"_blank\">f6_b16_h128_ll2_lr0.001_雙向_mae_p32_3fc_test00線性插值-C</a></strong> to <a href='https://wandb.ai/david___teng-national-central-university/aicup_power_v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/david___teng-national-central-university/aicup_power_v3' target=\"_blank\">https://wandb.ai/david___teng-national-central-university/aicup_power_v3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/david___teng-national-central-university/aicup_power_v3/runs/l7k80jst' target=\"_blank\">https://wandb.ai/david___teng-national-central-university/aicup_power_v3/runs/l7k80jst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidteng/anaconda3/envs/wsd/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/davidteng/aicup/electric/code/saved_models_v3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type   | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | lstm      | LSTM   | 535 K  | train\n",
      "1 | fc1       | Linear | 8.2 K  | train\n",
      "2 | fc2       | Linear | 231    | train\n",
      "3 | fc3       | Linear | 8      | train\n",
      "4 | criterion | L1Loss | 0      | train\n",
      "---------------------------------------------\n",
      "544 K     Trainable params\n",
      "0         Non-trainable params\n",
      "544 K     Total params\n",
      "2.176     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidteng/anaconda3/envs/wsd/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidteng/anaconda3/envs/wsd/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 121/121 [00:02<00:00, 43.32it/s, v_num=0jst, train_loss_step=0.522, val_loss=0.333, train_loss_epoch=0.413]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 121/121 [00:02<00:00, 46.45it/s, v_num=0jst, train_loss_step=0.209, val_loss=0.319, train_loss_epoch=0.333]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 121/121 [00:02<00:00, 46.64it/s, v_num=0jst, train_loss_step=0.0918, val_loss=0.310, train_loss_epoch=0.311]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 121/121 [00:02<00:00, 46.41it/s, v_num=0jst, train_loss_step=0.476, val_loss=0.307, train_loss_epoch=0.310] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 121/121 [00:02<00:00, 46.70it/s, v_num=0jst, train_loss_step=0.333, val_loss=0.298, train_loss_epoch=0.303]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 121/121 [00:02<00:00, 46.62it/s, v_num=0jst, train_loss_step=0.302, val_loss=0.295, train_loss_epoch=0.300]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 121/121 [00:02<00:00, 46.57it/s, v_num=0jst, train_loss_step=0.324, val_loss=0.294, train_loss_epoch=0.291]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 121/121 [00:02<00:00, 46.61it/s, v_num=0jst, train_loss_step=0.462, val_loss=0.292, train_loss_epoch=0.286]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 121/121 [00:02<00:00, 47.14it/s, v_num=0jst, train_loss_step=0.136, val_loss=0.283, train_loss_epoch=0.281]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 121/121 [00:02<00:00, 46.75it/s, v_num=0jst, train_loss_step=0.285, val_loss=0.266, train_loss_epoch=0.276]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 121/121 [00:02<00:00, 46.21it/s, v_num=0jst, train_loss_step=0.153, val_loss=0.259, train_loss_epoch=0.264]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 121/121 [00:02<00:00, 46.25it/s, v_num=0jst, train_loss_step=0.168, val_loss=0.256, train_loss_epoch=0.258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 121/121 [00:02<00:00, 46.26it/s, v_num=0jst, train_loss_step=0.228, val_loss=0.252, train_loss_epoch=0.260]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 121/121 [00:02<00:00, 47.46it/s, v_num=0jst, train_loss_step=0.309, val_loss=0.246, train_loss_epoch=0.250]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 121/121 [00:02<00:00, 46.38it/s, v_num=0jst, train_loss_step=0.123, val_loss=0.240, train_loss_epoch=0.241]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 121/121 [00:02<00:00, 46.72it/s, v_num=0jst, train_loss_step=0.257, val_loss=0.240, train_loss_epoch=0.236] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 121/121 [00:02<00:00, 46.35it/s, v_num=0jst, train_loss_step=0.246, val_loss=0.236, train_loss_epoch=0.235]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 121/121 [00:02<00:00, 46.70it/s, v_num=0jst, train_loss_step=0.150, val_loss=0.233, train_loss_epoch=0.234]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 121/121 [00:02<00:00, 46.78it/s, v_num=0jst, train_loss_step=0.239, val_loss=0.227, train_loss_epoch=0.227]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 121/121 [00:02<00:00, 47.00it/s, v_num=0jst, train_loss_step=0.137, val_loss=0.220, train_loss_epoch=0.210] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 121/121 [00:02<00:00, 46.87it/s, v_num=0jst, train_loss_step=0.247, val_loss=0.218, train_loss_epoch=0.203]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 121/121 [00:02<00:00, 46.22it/s, v_num=0jst, train_loss_step=0.143, val_loss=0.217, train_loss_epoch=0.201]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 121/121 [00:02<00:00, 46.38it/s, v_num=0jst, train_loss_step=0.315, val_loss=0.210, train_loss_epoch=0.201]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 121/121 [00:02<00:00, 46.41it/s, v_num=0jst, train_loss_step=0.239, val_loss=0.207, train_loss_epoch=0.196] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 121/121 [00:02<00:00, 46.33it/s, v_num=0jst, train_loss_step=0.0953, val_loss=0.207, train_loss_epoch=0.192]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 121/121 [00:02<00:00, 46.52it/s, v_num=0jst, train_loss_step=0.236, val_loss=0.206, train_loss_epoch=0.186] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 121/121 [00:02<00:00, 46.14it/s, v_num=0jst, train_loss_step=0.209, val_loss=0.206, train_loss_epoch=0.184]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102: 100%|██████████| 121/121 [00:02<00:00, 46.60it/s, v_num=0jst, train_loss_step=0.141, val_loss=0.199, train_loss_epoch=0.186]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124: 100%|██████████| 121/121 [00:02<00:00, 46.24it/s, v_num=0jst, train_loss_step=0.206, val_loss=0.198, train_loss_epoch=0.174] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133: 100%|██████████| 121/121 [00:02<00:00, 46.54it/s, v_num=0jst, train_loss_step=0.229, val_loss=0.195, train_loss_epoch=0.171] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137: 100%|██████████| 121/121 [00:02<00:00, 45.94it/s, v_num=0jst, train_loss_step=0.200, val_loss=0.193, train_loss_epoch=0.166] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145: 100%|██████████| 121/121 [00:02<00:00, 46.60it/s, v_num=0jst, train_loss_step=0.189, val_loss=0.193, train_loss_epoch=0.163] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153: 100%|██████████| 121/121 [00:02<00:00, 46.20it/s, v_num=0jst, train_loss_step=0.111, val_loss=0.193, train_loss_epoch=0.159] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157: 100%|██████████| 121/121 [00:02<00:00, 46.44it/s, v_num=0jst, train_loss_step=0.259, val_loss=0.191, train_loss_epoch=0.159] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192: 100%|██████████| 121/121 [00:02<00:00, 46.70it/s, v_num=0jst, train_loss_step=0.181, val_loss=0.191, train_loss_epoch=0.142] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193: 100%|██████████| 121/121 [00:02<00:00, 47.27it/s, v_num=0jst, train_loss_step=0.172, val_loss=0.190, train_loss_epoch=0.142] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194: 100%|██████████| 121/121 [00:02<00:00, 46.85it/s, v_num=0jst, train_loss_step=0.117, val_loss=0.190, train_loss_epoch=0.142] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201: 100%|██████████| 121/121 [00:02<00:00, 46.25it/s, v_num=0jst, train_loss_step=0.117, val_loss=0.187, train_loss_epoch=0.139] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233: 100%|██████████| 121/121 [00:03<00:00, 32.52it/s, v_num=0jst, train_loss_step=0.132, val_loss=0.187, train_loss_epoch=0.128] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244: 100%|██████████| 121/121 [00:02<00:00, 47.18it/s, v_num=0jst, train_loss_step=0.0811, val_loss=0.187, train_loss_epoch=0.125]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253: 100%|██████████| 121/121 [00:02<00:00, 46.11it/s, v_num=0jst, train_loss_step=0.0662, val_loss=0.187, train_loss_epoch=0.124]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303: 100%|██████████| 121/121 [00:03<00:00, 33.32it/s, v_num=0jst, train_loss_step=0.111, val_loss=0.190, train_loss_epoch=0.114] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 50 records. Best score: 0.187. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303: 100%|██████████| 121/121 [00:03<00:00, 33.27it/s, v_num=0jst, train_loss_step=0.111, val_loss=0.190, train_loss_epoch=0.114]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
